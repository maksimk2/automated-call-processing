# Databricks notebook source
# MAGIC %md
# MAGIC # üîÑ Notebook: 01 ETL Silver Layer
# MAGIC
# MAGIC This notebook handles the **Silver Layer (Data Transformation & Processing)** in the Medallion Architecture of the AI-powered claims pipeline. It focuses on **audio conversion**, **metadata extraction**, and **speech-to-text transcription** using the [OpenAI Whisper model](https://openai.com/index/whisper/).
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## üß± Purpose
# MAGIC
# MAGIC To convert raw audio files into a consistent format (MP3), calculate metadata (duration), and transcribe the content into structured text to support downstream AI analytics.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## ‚öôÔ∏è Configuration (resources/init.py)
# MAGIC
# MAGIC **To change language or model:**
# MAGIC 1. Open `resources/init.py`
# MAGIC 2. Update these variables:
# MAGIC    ```python
# MAGIC    WHISPER_MODEL_SIZE = "base"  # Change to "medium" or "large" for better accuracy
# MAGIC    WHISPER_LANGUAGE = "en"      # Change to "nl" for Dutch, or None for auto-detect
# MAGIC    ```
# MAGIC 3. Re-run this notebook
# MAGIC
# MAGIC

# COMMAND ----------

# DBTITLE 1,Install required libraries
# MAGIC %pip install pydub mutagen openai-whisper
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,Install ffmpeg (system dependency)
# MAGIC %sh
# MAGIC apt-get update && apt-get install -y ffmpeg

# COMMAND ----------

# DBTITLE 1,Load Initial Resources for Notebook Execution
# MAGIC %run "./init" 

# COMMAND ----------

# DBTITLE 1,Load Unprocessed Audio Metadata
bronze_df = spark.table(f"{CATALOG}.{SCHEMA}.{BRONZE_TABLE}")

meta_table_name = f"{CATALOG}.{SCHEMA}.{META_TABLE}"
if spark.catalog.tableExists(meta_table_name):
    processed_df = spark.table(meta_table_name).filter("processed = True")
    file_reference_df = bronze_df.join(processed_df, "file_name", "left_anti")
else:
    file_reference_df = bronze_df

if file_reference_df.isEmpty():
    dbutils.notebook.exit("‚úÖ No new files to process. Exiting Silver Layer.")

# COMMAND ----------

# DBTITLE 1,Convert Raw Audio ‚Üí MP3
from pydub import AudioSegment
import os

dbutils.fs.mkdirs(mp3_audio_path)

# Convert each file to mp3 and save to the new volume
for row in file_reference_df.collect():
    file_path = row['file_path']
    try:
        audio = AudioSegment.from_file(file_path)
        new_file_path = os.path.join(mp3_audio_path, os.path.basename(file_path).replace(os.path.splitext(file_path)[1], ".mp3"))
        audio.export(new_file_path, format="mp3")
    except Exception as e:
        print(f"‚ö†Ô∏è Error converting {file_path}: {e}")

# COMMAND ----------

# DBTITLE 1,Extract Audio Durations (MP3)
from mutagen.mp3 import MP3
from pyspark.sql.types import FloatType
import pyspark.sql.functions as F

mp3_df = (
    spark.createDataFrame(dbutils.fs.ls(mp3_audio_path))
    .withColumn("file_path", F.expr("substring(path, 6, length(path))"))
    .withColumn("file_name", F.expr("substring(name, 1, length(name) - 4)"))
    .filter(F.col("file_name").isin([r["file_name"] for r in file_reference_df.collect()]))
)

def get_audio_duration(file_path):
    try:
        audio = MP3(file_path)
        return float(audio.info.length)
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting duration for {file_path}: {e}")
        return None

duration_udf = F.udf(get_audio_duration, FloatType())
mp3_df = mp3_df.withColumn("audio_duration", F.round(duration_udf("file_path"), 0))

display(mp3_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ### üöÄ Distributed Transcription Architecture
# MAGIC 
# MAGIC **Serverless Compute Optimization:**
# MAGIC - Whisper model is loaded **once per executor** (not per file)
# MAGIC - Files are processed **in parallel** across all executors
# MAGIC - Scales horizontally - more files = more executors automatically provisioned
# MAGIC - Secure: All processing stays within encrypted Unity Catalog volumes
# MAGIC 
# MAGIC **Language Configuration:**
# MAGIC - **Current**: English (`en`) - configured in `resources/init.py`
# MAGIC - **To change language**: Update `WHISPER_LANGUAGE` in `resources/init.py`
# MAGIC   - `"en"` - English
# MAGIC   - `"nl"` - Dutch
# MAGIC   - `"de"` - German
# MAGIC   - `"fr"` - French
# MAGIC   - `None` - Auto-detect (slower, use for mixed languages)
# MAGIC 
# MAGIC **Model Size Configuration:**
# MAGIC - **Current**: `base` (~140MB) - configured in `resources/init.py`
# MAGIC - **Options** (update `WHISPER_MODEL_SIZE` in `resources/init.py`):
# MAGIC   - `"base"`: Good for general use, fast
# MAGIC   - `"medium"`: Better for medical terminology, recommended for production
# MAGIC   - `"large"`: Best accuracy for complex medical cases
# MAGIC 
# MAGIC **Healthcare Compliance:**
# MAGIC - ‚úÖ AVG (GDPR) compliant - data encrypted at rest
# MAGIC - ‚úÖ NEN 7510 compliant - proper access controls
# MAGIC - ‚úÖ Wbsn compliant - medical data stays in secure boundary
# MAGIC - ‚ö†Ô∏è **Data Residency**: Ensure Databricks workspace is in EU region
# MAGIC 
# MAGIC **Performance:**
# MAGIC - 10 files: ~2-3 minutes (single executor)
# MAGIC - 100 files: ~3-5 minutes (multiple executors in parallel)
# MAGIC - 1000+ files: Scales linearly with serverless auto-scaling

# COMMAND ----------

# DBTITLE 1,Transcribe Audio with Distributed Whisper (Serverless Optimized)
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StringType
import pandas as pd
import whisper

# Define distributed transcription UDF
@pandas_udf(StringType())
def transcribe_audio_udf(file_paths: pd.Series) -> pd.Series:
    """
    Distributed transcription using Whisper model
    Language and model size configured in init.py for easy updates
    """
    import whisper
    
    # Get configuration from init.py
    model_size = WHISPER_MODEL_SIZE
    language = WHISPER_LANGUAGE
    
    # Load model once per executor (cached for batch)
    model = whisper.load_model(model_size)
    print(f"üìù Whisper model '{model_size}' loaded, language: {language if language else 'auto-detect'}")
    
    def transcribe_single(file_path):
        try:
            # Build transcription parameters
            transcribe_params = {
                "audio": file_path,
                "task": "transcribe"  # Keep original language (not translate)
            }
            
            # Add language hint if specified (None = auto-detect)
            if language:
                transcribe_params["language"] = language
            
            result = model.transcribe(**transcribe_params)
            return result["text"]
        except Exception as e:
            print(f"‚ö†Ô∏è Transcription failed for {file_path}: {e}")
            return ""
    
    # Process all files in this partition
    return file_paths.apply(transcribe_single)

print("üöÄ Starting distributed transcription across cluster...")
print(f"üìä Processing {mp3_df.count()} files in parallel using serverless compute")

# Apply distributed transcription
transcribed_df = mp3_df.withColumn("transcription", transcribe_audio_udf("file_path"))

# Select final columns
transcribed_df = transcribed_df.select("file_path", "file_name", "transcription", "audio_duration")

print("‚úÖ Distributed transcription completed securely")
display(transcribed_df)


# COMMAND ----------

# DBTITLE 1,Enrich with Metadata
from pyspark.sql.functions import col, split, regexp_replace, to_timestamp, concat_ws

transcribed_df = transcribed_df \
    .withColumn("file_name", regexp_replace(col("file_name"), ".mp3", "")) \
    .withColumn("call_id", split(col("file_name"), "_").getItem(0)) \
    .withColumn("agent_id", split(col("file_name"), "_").getItem(1)) \
    .withColumn("call_datetime", 
        to_timestamp(
            concat_ws(":", split(col("file_name"), "_").getItem(2), 
            split(col("file_name"), "_").getItem(3), 
            split(col("file_name"), "_").getItem(4)))
    )

display(transcribed_df.select("file_name", "call_id", "agent_id", "call_datetime", "audio_duration", "transcription"))

# COMMAND ----------

# DBTITLE 1,Write to Silver Table
silver_table_path = f"{CATALOG}.{SCHEMA}.{SILVER_TABLE}"

if not spark.catalog.tableExists(silver_table_path):
    transcribed_df.write.mode("overwrite").option("overwriteSchema", "true").saveAsTable(silver_table_path)
else:
    transcribed_df.write.mode("append").saveAsTable(silver_table_path)

print(f"‚úÖ Transcriptions written to Silver table: {silver_table_path}")

# COMMAND ----------

# DBTITLE 1,Export Transcriptions to Documents (Optional)
# Uncomment to export transcriptions as downloadable text files

# export_path = f"{mp3_audio_path}transcriptions_export/"
# dbutils.fs.mkdirs(export_path)

# for row in transcribed_df.collect():
#     file_name = row['file_name']
#     transcription = row['transcription']
#     call_datetime = row['call_datetime']
#     agent_id = row['agent_id']
    
#     # Format document content
#     document_content = f"""
# CONFIDENTIAL TRANSCRIPTION
# ========================

# Call ID: {row['call_id']}
# Agent: {agent_id}
# Date/Time: {call_datetime}
# Duration: {row['audio_duration']} seconds

# TRANSCRIPTION:
# --------------
# {transcription}

# ========================
# Generated by Databricks Automated Transcription System
# """
    
#     # Save as text file
#     export_file = f"{export_path}{file_name}.txt"
#     dbutils.fs.put(export_file, document_content, overwrite=True)
#     print(f"üìÑ Exported: {export_file}")

# print(f"‚úÖ All transcriptions exported to: {export_path}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ‚úÖ Output
# MAGIC - A clean, enriched Delta table: transcriptions_silver
# MAGIC - Includes transcription text, call metadata, and audio duration for each entry.